apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: ceph
  namespace: ceph
spec:
  cephConfig:
    global:
      osd_pool_default_size: "1"
      mon_warn_on_pool_no_redundancy: "false"
      bdev_flock_retry: "20"
      bluefs_buffered_io: "false"
      mon_data_avail_warn: "10"
  cephVersion:
    image: quay.io/ceph/ceph:v19.2
  # clusterMetadata:
  #   namespace: ceph
  crashCollector:
    disable: true
  dashboard:
    enabled: true
    # allowUnsupported: true
  dataDirHostPath: /var/lib/rook
  disruptionManagement:
    managePodBudgets: true
  healthCheck:
    daemonHealth:
      mon:
        interval: 45s
        timeout: 600s
  mgr:
    count: 1
    allowMultiplePerNode: true
    modules:
      - name: rook
        enabled: true
  mon:
    count: 1
    allowMultiplePerNode: true
    volumeClaimTemplate:
      spec:
        storageClassName: local-path
        resources:
          requests:
            storage: 10Gi
  monitoring:
    enabled: true
  # preparePlacement:
  #   all:
  #     tolerations:
  #       - effect: NoSchedule
  #         key: node-role.kubernetes.io/control-plane
  #         operator: Exists
  #       - effect: NoSchedule
  #         key: node-role.kubernetes.io/master
  #         operator: Exists
  priorityClassNames:
    all: system-node-critical
    mgr: system-cluster-critical
  # test environments can skip ok-to-stop checks during upgrades
  skipUpgradeChecks: true
  storage:
    allowDeviceClassUpdate: true
    allowOsdCrushWeightUpdate: false
    #deviceFilter:
    #config:
    #  deviceClass: testclass
    scheduleAlways: true
    storageClassDeviceSets:
      - name: local-path
        count: 1
        # whether to encrypt the deviceSet or not
        encrypted: false
        # Since the OSDs could end up on any node, an effort needs to be made to spread the OSDs
        # across nodes as much as possible. Unfortunately the pod anti-affinity breaks down
        # as soon as you have more than one OSD per node. The topology spread constraints will
        # give us an even spread on K8s 1.18 or newer.
        placement:
          topologySpreadConstraints:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd
              maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: ScheduleAnyway
        # IMPORTANT: If volumes specified by the storageClassName are not portable across nodes
        # this needs to be set to false. For example, if using the local storage provisioner
        # this should be false.
        portable: false
        preparePlacement:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-osd
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-osd-prepare
                  topologyKey: kubernetes.io/hostname
                weight: 100
          topologySpreadConstraints:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd-prepare
              maxSkew: 1
              # IMPORTANT: If you don't have zone labels, change this to another key such as kubernetes.io/hostname
              # topologyKey: topology.kubernetes.io/zone
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: DoNotSchedule
        # Certain storage class in the Cloud are slow
        # Rook can configure the OSD running on PVC to accommodate that by tuning some of the Ceph internal
        # Currently, "gp2-csi" has been identified as such
        tuneDeviceClass: false
        # Certain storage class in the Cloud are fast
        # Rook can configure the OSD running on PVC to accommodate that by tuning some of the Ceph internal
        # Currently, "managed-premium" has been identified as such
        tuneFastDeviceClass: false
        volumeClaimTemplates:
          - metadata:
              name: data
              # set a different CRUSH device class on the OSD than the one detected by Ceph
              # annotations:
              #   crushDeviceClass: hybrid
            spec:
              resources:
                requests:
                  storage: 10Gi
              # IMPORTANT: Change the storage class depending on your environment
              storageClassName: local-storage
              volumeMode: Block
              accessModes:
                - ReadWriteOnce
    # useAllNodes: true
    # useAllDevices: true
